{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Part_3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.12"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRlf-VjoOZ8O"
      },
      "source": [
        "# Part 3 - Text analysis and ethics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tU8BnCXIOZ8T"
      },
      "source": [
        "# 3.a Computing PMI\n",
        "\n",
        "In this assessment you are tasked to discover strong associations between concepts in Airbnb reviews. The starter code we provide in this notebook is for orientation only. The below imports are enough to implement a valid answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_BJYvjpOZ8U"
      },
      "source": [
        "### Imports, data loading and helper functions\n",
        "\n",
        "We first connect our google drive, import pandas, numpy and some useful nltk and collections modules, then load the dataframe and define a function for printing the current time, useful to log our progress in some of the tasks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kS7mACxfcjPI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e744f91c-6700-4c15-e536-6dda44747d30"
      },
      "source": [
        "!pip install ipython-autotime\n",
        " \n",
        "%load_ext autotime"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ipython-autotime in /usr/local/lib/python3.7/dist-packages (0.3.1)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from ipython-autotime) (5.5.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (57.0.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (4.4.2)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (5.0.5)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (1.0.18)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (2.6.1)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (0.8.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (0.7.5)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (4.8.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets>=4.2->ipython->ipython-autotime) (0.2.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->ipython-autotime) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->ipython-autotime) (0.2.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect; sys_platform != \"win32\"->ipython->ipython-autotime) (0.7.0)\n",
            "time: 1.96 ms (started: 2021-06-04 05:01:01 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0z_s4GpwOZ8U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44eb463c-20c7-4f3a-e1b0-0474eb3f0362"
      },
      "source": [
        "import pandas as pd\n",
        "from nltk.tag import pos_tag\n",
        "import re\n",
        "from collections import defaultdict,Counter\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import os\n",
        "import scipy.sparse\n",
        "import string\n",
        "tqdm.pandas()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 702 ms (started: 2021-06-04 05:01:01 +00:00)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tqdm/std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
            "  from pandas import Panel\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFP8c6HlPF_-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ec4bee8-5e0d-4440-e1fd-e4da4d45d345"
      },
      "source": [
        "# nltk imports, note that these outputs may be different if you are using colab or local jupyter notebooks\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "time: 102 ms (started: 2021-06-04 05:01:01 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JOWJqE9Pq5V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccb07c2e-4b6d-4ef1-a236-668c210d81a3"
      },
      "source": [
        "# load stopwords\n",
        "sw = set(stopwords.words('english'))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 4.23 ms (started: 2021-06-04 05:01:02 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mw9FDi4T4kZU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22d66a0f-80ad-4e48-c1cf-736c5c1c3627"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "time: 2.53 ms (started: 2021-06-04 05:01:02 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVD9Q3AxOZ8V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c385a4b-ac6f-4901-816c-64470e633900"
      },
      "source": [
        "p = 'some_directory'\n",
        "df = pd.read_csv(os.path.join(p,'/content/drive/MyDrive/Computational Data science/reviews.csv'))\n",
        "# deal with empty reviews\n",
        "df.comments = df.comments.fillna('')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 2.53 s (started: 2021-06-04 06:36:44 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNgPCqMPOZ8V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "10c97407-3ba5-41e5-916f-78bf1d47aca3"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>listing_id</th>\n",
              "      <th>id</th>\n",
              "      <th>date</th>\n",
              "      <th>reviewer_id</th>\n",
              "      <th>reviewer_name</th>\n",
              "      <th>comments</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2818</td>\n",
              "      <td>1191</td>\n",
              "      <td>2009-03-30</td>\n",
              "      <td>10952</td>\n",
              "      <td>Lam</td>\n",
              "      <td>Daniel is really cool. The place was nice and ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2818</td>\n",
              "      <td>1771</td>\n",
              "      <td>2009-04-24</td>\n",
              "      <td>12798</td>\n",
              "      <td>Alice</td>\n",
              "      <td>Daniel is the most amazing host! His place is ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2818</td>\n",
              "      <td>1989</td>\n",
              "      <td>2009-05-03</td>\n",
              "      <td>11869</td>\n",
              "      <td>Natalja</td>\n",
              "      <td>We had such a great time in Amsterdam. Daniel ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2818</td>\n",
              "      <td>2797</td>\n",
              "      <td>2009-05-18</td>\n",
              "      <td>14064</td>\n",
              "      <td>Enrique</td>\n",
              "      <td>Very professional operation. Room is very clea...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2818</td>\n",
              "      <td>3151</td>\n",
              "      <td>2009-05-25</td>\n",
              "      <td>17977</td>\n",
              "      <td>Sherwin</td>\n",
              "      <td>Daniel is highly recommended.  He provided all...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   listing_id  ...                                           comments\n",
              "0        2818  ...  Daniel is really cool. The place was nice and ...\n",
              "1        2818  ...  Daniel is the most amazing host! His place is ...\n",
              "2        2818  ...  We had such a great time in Amsterdam. Daniel ...\n",
              "3        2818  ...  Very professional operation. Room is very clea...\n",
              "4        2818  ...  Daniel is highly recommended.  He provided all...\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        },
        {
          "output_type": "stream",
          "text": [
            "time: 22.5 ms (started: 2021-06-04 06:36:48 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_9leP4VOZ8W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bd7e17d-ae8c-45c8-a374-ade2a2864852"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(452143, 6)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        },
        {
          "output_type": "stream",
          "text": [
            "time: 5.76 ms (started: 2021-06-04 06:36:50 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJfVvyXyPYS4"
      },
      "source": [
        "### 3.a1 - Process reviews\n",
        "\n",
        "What to implement: A `function process_reviews(df)` that will take as input the original dataframe and will return it with three additional columns: `tokenized`, `tagged` and `lower_tagged`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZP00pmliHMN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc930e70-6595-4496-8e59-bb19d4c924e2"
      },
      "source": [
        "def process_reviews(df):\n",
        "  ''' This function perform task of tokenizing ,tagging and transform the tagged word into lower case.\n",
        "  argument = DataFrame\n",
        "  return = DataFrame with three additional columns''' \n",
        "\n",
        "  # word tokenizing and add new column\n",
        "  df['tokenized'] = df['comments'].apply(word_tokenize)\n",
        "\n",
        "  # tagging using pos_tag\n",
        "  tag = []\n",
        "  for comment in df.comments:\n",
        "    tag.append(pos_tag(comment.translate(str.maketrans('', '', string.punctuation)).split()))\n",
        "  df[\"tagged\"] = tag\n",
        "\n",
        "  # converting all the tagged words to lower to reduce memory usuage.\n",
        "  lower_tag = []\n",
        "  for tag in df.tagged:\n",
        "    lwr_tag = []\n",
        "    for word in tag:\n",
        "      wrd = (word[0].lower(), word[1])\n",
        "      lwr_tag.append(wrd)\n",
        "    lower_tag.append(lwr_tag)\n",
        "  df[\"lower_tagged\"] = lower_tag\n",
        "  return df"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 13.4 ms (started: 2021-06-04 05:01:04 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGYB8gx5Qq-P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60842d50-898c-4ef0-812c-88048959bbd8"
      },
      "source": [
        "df = process_reviews(df)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 10min 44s (started: 2021-06-04 05:01:04 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZp7ZRnzpI47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 606
        },
        "outputId": "1b8efa7b-5da5-42eb-ce2d-31644e86221a"
      },
      "source": [
        "df"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>listing_id</th>\n",
              "      <th>id</th>\n",
              "      <th>date</th>\n",
              "      <th>reviewer_id</th>\n",
              "      <th>reviewer_name</th>\n",
              "      <th>comments</th>\n",
              "      <th>tokenized</th>\n",
              "      <th>tagged</th>\n",
              "      <th>lower_tagged</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2818</td>\n",
              "      <td>1191</td>\n",
              "      <td>2009-03-30</td>\n",
              "      <td>10952</td>\n",
              "      <td>Lam</td>\n",
              "      <td>Daniel is really cool. The place was nice and ...</td>\n",
              "      <td>[Daniel, is, really, cool, ., The, place, was,...</td>\n",
              "      <td>[(Daniel, NNP), (is, VBZ), (really, RB), (cool...</td>\n",
              "      <td>[(daniel, NNP), (is, VBZ), (really, RB), (cool...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2818</td>\n",
              "      <td>1771</td>\n",
              "      <td>2009-04-24</td>\n",
              "      <td>12798</td>\n",
              "      <td>Alice</td>\n",
              "      <td>Daniel is the most amazing host! His place is ...</td>\n",
              "      <td>[Daniel, is, the, most, amazing, host, !, His,...</td>\n",
              "      <td>[(Daniel, NNP), (is, VBZ), (the, DT), (most, R...</td>\n",
              "      <td>[(daniel, NNP), (is, VBZ), (the, DT), (most, R...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2818</td>\n",
              "      <td>1989</td>\n",
              "      <td>2009-05-03</td>\n",
              "      <td>11869</td>\n",
              "      <td>Natalja</td>\n",
              "      <td>We had such a great time in Amsterdam. Daniel ...</td>\n",
              "      <td>[We, had, such, a, great, time, in, Amsterdam,...</td>\n",
              "      <td>[(We, PRP), (had, VBD), (such, JJ), (a, DT), (...</td>\n",
              "      <td>[(we, PRP), (had, VBD), (such, JJ), (a, DT), (...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2818</td>\n",
              "      <td>2797</td>\n",
              "      <td>2009-05-18</td>\n",
              "      <td>14064</td>\n",
              "      <td>Enrique</td>\n",
              "      <td>Very professional operation. Room is very clea...</td>\n",
              "      <td>[Very, professional, operation, ., Room, is, v...</td>\n",
              "      <td>[(Very, RB), (professional, JJ), (operation, N...</td>\n",
              "      <td>[(very, RB), (professional, JJ), (operation, N...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2818</td>\n",
              "      <td>3151</td>\n",
              "      <td>2009-05-25</td>\n",
              "      <td>17977</td>\n",
              "      <td>Sherwin</td>\n",
              "      <td>Daniel is highly recommended.  He provided all...</td>\n",
              "      <td>[Daniel, is, highly, recommended, ., He, provi...</td>\n",
              "      <td>[(Daniel, NNP), (is, VBZ), (highly, RB), (reco...</td>\n",
              "      <td>[(daniel, NNP), (is, VBZ), (highly, RB), (reco...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199995</th>\n",
              "      <td>8560237</td>\n",
              "      <td>141676637</td>\n",
              "      <td>2017-04-04</td>\n",
              "      <td>17391423</td>\n",
              "      <td>Paul</td>\n",
              "      <td>It was a pleasure to stay at Marja's home. The...</td>\n",
              "      <td>[It, was, a, pleasure, to, stay, at, Marja, 's...</td>\n",
              "      <td>[(It, PRP), (was, VBD), (a, DT), (pleasure, NN...</td>\n",
              "      <td>[(it, PRP), (was, VBD), (a, DT), (pleasure, NN...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199996</th>\n",
              "      <td>8560237</td>\n",
              "      <td>146877857</td>\n",
              "      <td>2017-04-24</td>\n",
              "      <td>114705007</td>\n",
              "      <td>Dennis</td>\n",
              "      <td>Ruhig und durch den Anschluss zur Metro in 7 m...</td>\n",
              "      <td>[Ruhig, und, durch, den, Anschluss, zur, Metro...</td>\n",
              "      <td>[(Ruhig, NNP), (und, JJ), (durch, NN), (den, N...</td>\n",
              "      <td>[(ruhig, NNP), (und, JJ), (durch, NN), (den, N...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199997</th>\n",
              "      <td>8560237</td>\n",
              "      <td>147475181</td>\n",
              "      <td>2017-04-27</td>\n",
              "      <td>108433229</td>\n",
              "      <td>Vanessa</td>\n",
              "      <td>Eine sehr zuvorkommende und liebe Familie, bei...</td>\n",
              "      <td>[Eine, sehr, zuvorkommende, und, liebe, Famili...</td>\n",
              "      <td>[(Eine, NNP), (sehr, NN), (zuvorkommende, NN),...</td>\n",
              "      <td>[(eine, NNP), (sehr, NN), (zuvorkommende, NN),...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199998</th>\n",
              "      <td>8560237</td>\n",
              "      <td>158076178</td>\n",
              "      <td>2017-06-05</td>\n",
              "      <td>123933418</td>\n",
              "      <td>Megan</td>\n",
              "      <td>Very lovely place to stay, very clean and a gr...</td>\n",
              "      <td>[Very, lovely, place, to, stay, ,, very, clean...</td>\n",
              "      <td>[(Very, RB), (lovely, RB), (place, NN), (to, T...</td>\n",
              "      <td>[(very, RB), (lovely, RB), (place, NN), (to, T...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199999</th>\n",
              "      <td>8560237</td>\n",
              "      <td>161246069</td>\n",
              "      <td>2017-06-17</td>\n",
              "      <td>63364873</td>\n",
              "      <td>Indigo</td>\n",
              "      <td>Marja has a beautiful home in a great location...</td>\n",
              "      <td>[Marja, has, a, beautiful, home, in, a, great,...</td>\n",
              "      <td>[(Marja, NNP), (has, VBZ), (a, DT), (beautiful...</td>\n",
              "      <td>[(marja, NNP), (has, VBZ), (a, DT), (beautiful...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>200000 rows × 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        listing_id  ...                                       lower_tagged\n",
              "0             2818  ...  [(daniel, NNP), (is, VBZ), (really, RB), (cool...\n",
              "1             2818  ...  [(daniel, NNP), (is, VBZ), (the, DT), (most, R...\n",
              "2             2818  ...  [(we, PRP), (had, VBD), (such, JJ), (a, DT), (...\n",
              "3             2818  ...  [(very, RB), (professional, JJ), (operation, N...\n",
              "4             2818  ...  [(daniel, NNP), (is, VBZ), (highly, RB), (reco...\n",
              "...            ...  ...                                                ...\n",
              "199995     8560237  ...  [(it, PRP), (was, VBD), (a, DT), (pleasure, NN...\n",
              "199996     8560237  ...  [(ruhig, NNP), (und, JJ), (durch, NN), (den, N...\n",
              "199997     8560237  ...  [(eine, NNP), (sehr, NN), (zuvorkommende, NN),...\n",
              "199998     8560237  ...  [(very, RB), (lovely, RB), (place, NN), (to, T...\n",
              "199999     8560237  ...  [(marja, NNP), (has, VBZ), (a, DT), (beautiful...\n",
              "\n",
              "[200000 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        },
        {
          "output_type": "stream",
          "text": [
            "time: 365 ms (started: 2021-06-04 05:11:48 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUaH-yNlQRL9"
      },
      "source": [
        "### 3.a2 - Create a vocabulary\n",
        "\n",
        "What to implement: A function `get_vocab(df)` which takes as input the DataFrame generated in step 1.c, and returns two lists, one for the 1,000 most frequent center words (nouns) and one for the 1,000 most frequent context words (either verbs or adjectives). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAg6VRwdQQmg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b21c6cfe-1975-4299-dc6a-902f4378abf7"
      },
      "source": [
        "def get_vocab(df):\n",
        "  ''' This function generate two list, first one contains most common 1000 nouns and second one contains\n",
        "  most common 1000 verb/adjective.\n",
        "  argument: DataFrame\n",
        "  return: two lists.'''\n",
        "\n",
        "  New_list=[]\n",
        "  New_list1=[]\n",
        "  for i in range(len(df.lower_tagged)):\n",
        "    x=df[\"lower_tagged\"][i]\n",
        "    New_list.append(x)\n",
        "  Z=list(New_list)\n",
        "  New_list1=[]\n",
        "  for j in range(len(Z)):\n",
        "    t=Z[j]\n",
        "    for k in range(len(t)):\n",
        "      p=Z[j][k]\n",
        "      New_list1.append(p)\n",
        "  noun_list =[]\n",
        "  verb_list = []\n",
        "  noun_in = ['NNP','NNS','NNPS','NN']  # list of noun tags to check whether tag is noun or not\n",
        "  verb_in = ['JJS','JJ','JJR']         # list of verb/adjective tags to check whether tag is verb/adjective or not\n",
        "\n",
        "   # through for loop to check what is tag type.\n",
        "  for tok,tag in New_list1:\n",
        "    if tag in noun_in:\n",
        "      noun_list.append(tok)            # if tag is noun it will be added to noun list\n",
        "    elif tag in verb_in:\n",
        "      verb_list.append(tok)            # if tag is verb/adjective it will be added to adjective\n",
        "\n",
        "  # using Counter function to count the the occurance of a word.\n",
        "  noun_count = Counter(noun_list)\n",
        "  verb_count = Counter(verb_list)\n",
        "  noun_sorted = noun_count.most_common()\n",
        "  verb_sorted = verb_count.most_common()\n",
        "  New_verb=[]\n",
        "  New_noun=[]\n",
        "  for i in tqdm(range(len(verb_sorted))):\n",
        "    L=verb_sorted[i][0]\n",
        "    New_verb.append(L)\n",
        "  New_verb\n",
        "  for i in tqdm(range(len(noun_sorted))):\n",
        "    L=noun_sorted[i][0]\n",
        "    New_noun.append(L)\n",
        "\n",
        "  # removed puntuation who got tagged as noun or verb/adjective\n",
        "  New_noun = [''.join(c for c in s if c not in string.punctuation) for s in New_noun]\n",
        "  New_noun = [s for s in New_noun if s]\n",
        "  New_verb = [''.join(c for c in s if c not in string.punctuation) for s in New_verb]\n",
        "  New_verb = [s for s in New_verb if s]\n",
        "  print(len(New_noun),len(New_verb))\n",
        "\n",
        "  # To keep unique values in both the lists\n",
        "  final_verb=[]\n",
        "  for i in New_noun:\n",
        "    if i not in New_verb:\n",
        "      final_verb.append(i)\n",
        "\n",
        "  # Select most common 1000 Noun and verb/Adjective\n",
        "  cent_vocab = final_verb[:1000]\n",
        "  cont_vocab = New_verb[:1000]\n",
        "  return cent_vocab, cont_vocab"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 42.4 ms (started: 2021-06-04 05:11:48 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_R5l4IVSk9-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd92446b-2235-443b-b876-362ba15424cc"
      },
      "source": [
        "cent_vocab, cont_vocab = get_vocab(df)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 28134/28134 [00:00<00:00, 1343319.39it/s]\n",
            "100%|██████████| 110980/110980 [00:00<00:00, 1424199.25it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "110980 28134\n",
            "time: 50.2 s (started: 2021-06-04 05:11:48 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkqRGdQ_RUMg"
      },
      "source": [
        "### 3.a3 Count co-occurrences between center and context words\n",
        "\n",
        "What to implement: A function `get_coocs(df, center_vocab, context_vocab)` which takes as input the DataFrame generated in step 1, and the lists generated in step 2 and returns a dictionary of dictionaries, of the form in the example above. It is up to you how you define context (full review? per sentence? a sliding window of fixed size?), and how to deal with exceptional cases (center words occurring more than once, center and context words being part of your vocabulary because they are frequent both as a noun and as a verb, etc). Use comments in your code to justify your approach. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddnfCbQWRd5R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc64489c-f7d5-4af5-a30a-5154afb36fb1"
      },
      "source": [
        "def get_coocs(df, cent_vocab, cont_vocab):\n",
        "\n",
        "  '''This function get_coocs(df, center_vocab, context_vocab) which takes as \n",
        "  argument: the DataFrame generated in step 1, and the lists generated in step 2  \n",
        "  returns a dictionary of dictionaries, of the form in the example below\n",
        "  ‘A big restaurant served delicious food in big dishes’\n",
        "  {‘restaurant’: {‘big’: 2, ‘served’:1, ‘delicious’:1}} '''\n",
        "\n",
        "  X=(df[\"comments\"].apply(str.lower))\n",
        "  X=list(X)\n",
        "\n",
        "  \n",
        "  # sentence tokenisation\n",
        "  sentence_tokenize_lst=[]  \n",
        "  for i in (X):\n",
        "    k=sent_tokenize(i)\n",
        "    sentence_tokenize_lst.append(k)\n",
        "\n",
        "  # word tokenisation and save in a separate list\n",
        "  word_tokenizer_list=[] \n",
        "  for i in range(len(X)):\n",
        "    for j in range(len(sentence_tokenize_lst[i])):\n",
        "      T=word_tokenize(sentence_tokenize_lst[i][j])\n",
        "      word_tokenizer_list.append(T)\n",
        "\n",
        "# searching context vocab sentence wise and save in the list of find_context_vocab\n",
        "  find_context_vocab=[]\n",
        "  for i in range(len(word_tokenizer_list)):\n",
        "    context_vocab_list = []    # search context_vocab from each sentence and store in context_vocab_list\n",
        "    for j in cont_vocab:\n",
        "      if j in word_tokenizer_list[i]:\n",
        "        context_vocab_list.append(j)\n",
        "      if j is '.':\n",
        "       break\n",
        "    find_context_vocab.append(context_vocab_list)\n",
        "\n",
        "# count context vocab and save in the list of count_context_vocab\n",
        "  count_context_vocab=[]\n",
        "  for i in range(len(find_context_vocab)):\n",
        "    K=dict(Counter(find_context_vocab[i]))\n",
        "    count_context_vocab.append(K) \n",
        "\n",
        "\n",
        "# searching center vocab sentence wise and save in the list of find_context_vocab\n",
        "  find_center_vocab=[]\n",
        "  for i in (range(len(word_tokenizer_list))):\n",
        "    center_vocab_list = []\n",
        "    for j in cent_vocab:\n",
        "      if j in word_tokenizer_list[i]:\n",
        "        center_vocab_list.append(j)\n",
        "      if j is '.':\n",
        "        break\n",
        "    find_center_vocab.append(center_vocab_list)\n",
        "\n",
        "# count center vocab and save in the list of count_context_vocab\n",
        "  count_center_vocab=[]\n",
        "  for i in range(len(find_center_vocab)):\n",
        "    K=dict(Counter(find_center_vocab[i]))\n",
        "    count_center_vocab.append(K)\n",
        "\n",
        "  for i in range(len(count_center_vocab)):\n",
        "    for j in count_center_vocab[i]:\n",
        "      count_center_vocab[i][j]=count_context_vocab[i]\n",
        "\n",
        "# Make dictionary of dictionary each sentence wise.\n",
        "  new_list=[]\n",
        "  for i in range(len(count_center_vocab)):\n",
        "    final_dict={}\n",
        "    for j in count_center_vocab[i]:\n",
        "      final_dict[j]=count_center_vocab[i][j]\n",
        "      break\n",
        "    new_list.append(final_dict)\n",
        "\n",
        "# Remove all emnpty dictionary\n",
        "  while {} in new_list:\n",
        "    new_list.remove({})\n",
        "\n",
        "  # Remove all those dictionary in which there is key but no values coresponding to that key\n",
        "  Dictionary =[]\n",
        "  for i in range(len(new_list)):\n",
        "    if new_list[i].get(next(iter(new_list[i])))!={}:\n",
        "      Dictionary .append(new_list[i])\n",
        "\n",
        " # To convert sentence wise dictionary of dictionary  to whole  comment dictionary of dictionary.\n",
        "  dict1={}\n",
        "  for i in cent_vocab:\n",
        "    dict1[i]=[]\n",
        "  for i in range(len(Dictionary)):\n",
        "    for key in cent_vocab:\n",
        "      if key ==next(iter(Dictionary[i])):\n",
        "        dict1[key].append(Dictionary[i].get(key))\n",
        "  t_list=[]\n",
        "  for i in cent_vocab:\n",
        "    a_list=[]\n",
        "    for j in range(len(dict1[i])):\n",
        "      k=next(iter(dict1[i][j]))\n",
        "      a_list.append(k)\n",
        "    t_list.append(a_list)\n",
        "\n",
        "  # here  we make final dictionary of dictionary which having 1000 nouns( keys ) and there corresponding verb/adjective(values) according to whole comments.  \n",
        "  dict2=[]\n",
        "  coocs={}\n",
        "  for i,j in zip(cent_vocab,range(len(cent_vocab))):\n",
        "    k=dict(Counter(t_list[j]))\n",
        "    dict2.append(k)\n",
        "    coocs[i]=dict2[j]\n",
        "  \n",
        "  return coocs"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 89.6 ms (started: 2021-06-04 05:12:39 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTT_TOkaSoXL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9bd98ce-a2bb-4832-f14f-8604c641ded1"
      },
      "source": [
        "coocs = get_coocs(df, cent_vocab, cont_vocab)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 1h 19min 40s (started: 2021-06-04 05:12:39 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be6mOXqMRlt-"
      },
      "source": [
        "### 3.a4 Convert co-occurrence dictionary to 1000x1000 dataframe\n",
        "What to implement: A function called `cooc_dict2df(cooc_dict)`, which takes as input the dictionary of dictionaries generated in step 3 and returns a DataFrame where each row corresponds to one center word, and each column corresponds to one context word, and cells are their corresponding co-occurrence value. Some (x,y) pairs will never co-occur, you should have a 0 value for those cases. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6WuM5U7RsBJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad4d2693-bd48-44e6-d377-4c7eea1885fd"
      },
      "source": [
        "def cooc_dict2df(coocs):\n",
        "  ''' This function takes dictionary of dictionaries as argument and return a DataFrame'''\n",
        "  coocdf = pd.DataFrame.from_dict(coocs,orient = 'index',dtype=\"Int64\").fillna(0)\n",
        "  return coocdf"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 2.35 ms (started: 2021-06-04 06:40:40 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwAflxldSrbg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b58ed54d-24ac-48f1-bc4a-1a2c826078c9"
      },
      "source": [
        "'''\n",
        "Here the shape of data frame is (995,903) because of there are 5 such Noun there Value is Zero(0)(i.e. there corresponding no verb/adjective ).\n",
        "'''\n",
        "coocdf = cooc_dict2df(coocs)\n",
        "coocdf.shape"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(995, 903)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        },
        {
          "output_type": "stream",
          "text": [
            "time: 371 ms (started: 2021-06-04 06:40:41 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EWllWryR-QL"
      },
      "source": [
        "### 3.a5 Raw co-occurrences to PMI scores\n",
        "\n",
        "What to implement: A function `cooc2pmi(df)` that takes as input the DataFrame generated in step 4, and returns a new DataFrame with the same rows and columns, but with PMI scores instead of raw co-occurrence counts. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frTTs7-eSFHv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e08bc6e9-02de-4d55-c2f7-724d1a9a9ae2"
      },
      "source": [
        "def cooc2pmi(df):\n",
        "  ''' This function takes step 4 DataFrame as argunment and return new Dataframe with PMI score instead of raw co-occurence count.'''\n",
        "  row_totals = df.sum(axis=1).astype(float)         # take the total sum of all rows in dataframe\n",
        "  prob_cols_given_row = (df.T / row_totals).T       # calculating the probability of each index against total sum of rows\n",
        "  col_totals = df.sum(axis=0).astype(float)         # calculating sum of all rows.\n",
        "  prob_of_cols = col_totals / sum(col_totals)       # calculating the probability of each index against total sum of columns.\n",
        "  ratio = prob_cols_given_row / prob_of_cols        # calculating ratio\n",
        "  ratio[ratio==0] = 0.00001                         # replacing ratios that have zero value with 0.00001 to avoid mathematical error.\n",
        "  pmidf = np.log(ratio)                             # calculating log of ratio using numpy library log function\n",
        "  pmidf[pmidf < 0] = 0\n",
        "  pmidf = pmidf.fillna(0.00001)\n",
        "  return pmidf"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 5.94 ms (started: 2021-06-04 06:32:21 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGftXjXRSuQw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e209754-37c9-4606-87b6-b7a1739f9a00"
      },
      "source": [
        "pmidf = cooc2pmi(cooc_df)\n",
        "pmidf.shape\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(995, 903)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        },
        {
          "output_type": "stream",
          "text": [
            "time: 601 ms (started: 2021-06-04 06:38:09 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zaLRvjRySOYB"
      },
      "source": [
        "### 3.a6 Retrieve top-k context words, given a center word\n",
        "\n",
        "What to implement: A function `topk(df, center_word, N=10)` that takes as input: (1) the DataFrame generated in step 5, (2) a `center_word` (a string like `‘towels’`), and (3) an optional named argument called `N` with default value of 10; and returns a list of `N` strings, in order of their PMI score with the `center_word`. You do not need to handle cases for which the word `center_word` is not found in `df`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlKUP9SgSXlL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c810e6a2-5f32-4800-c248-ed2ec359cc8f"
      },
      "source": [
        "def topk(df, center_word, N=10):\n",
        "  ''' This function takes PMI score filled in Dataframe,center_word and an optional N argument with default value 10 as input \n",
        "  return a list of N strings in order of their PMI score with the center_word'''\n",
        "  # finding the top N PMI score with there index values.\n",
        "  top_words = df[center_word].sort_values(ascending = False).head(N)\n",
        "  return top_words"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 4.12 ms (started: 2021-06-04 06:32:22 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7C2SDbk5709R",
        "outputId": "aadae70b-3668-4528-aa65-80e748c541fa"
      },
      "source": [
        "\"machine\" in cent_vocab"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        },
        {
          "output_type": "stream",
          "text": [
            "time: 3.8 ms (started: 2021-06-04 06:32:22 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1I038zG1Sw62",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a4310e9-5047-4f13-d09a-09169eb1f47c"
      },
      "source": [
        "topk(pmidf,'coffee')"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pods           6.152465\n",
              "drinking       5.641640\n",
              "maker          5.428547\n",
              "iron           5.379275\n",
              "package        5.379275\n",
              "machine        4.815768\n",
              "quarters       4.360706\n",
              "pastries       4.333307\n",
              "arrangement    4.306639\n",
              "foods          4.206555\n",
              "Name: coffee, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        },
        {
          "output_type": "stream",
          "text": [
            "time: 12.3 ms (started: 2021-06-04 06:45:44 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hfcm5-7b0HKO"
      },
      "source": [
        "# 3.b Ethical, social and legal implications\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qd3uf-Qq4tYg"
      },
      "source": [
        "Local authorities in touristic hotspots like Amsterdam, NYC or Barcelona regulate the price of recreational apartments for rent to, among others, ensure that fair rent prices are kept for year-long residents. Consider your price recommender for hosts in Question 2c. Imagine that Airbnb recommends a new host to put the price of your flat at a price which is above the official regulations established by the local government. Upon inspection, you realize that the inflated price you have been recommended comes from many apartments in the area only being offered during an annual event which brings many tourists, and which causes prices to rise. \n",
        "\n",
        "In this context, critically reflect on the compliance of this recommender system with **one of the five actions** outlined in the **UK’s Data Ethics Framework**. You should prioritize the action that, in your opinion, is the weakest. Then, justify your choice by critically analyzing the three **key principles** outlined in the Framework, namely _transparency_, _accountability_ and _fairness_. Finally, you should propose and critically justify a solution that would improve the recommender system in at least one of these principles. You are strongly encouraged to follow a scholarly approach, e.g., with peer-reviewed references as support. \n",
        "\n",
        "Your report should be between 500 and 750 words long.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6QJyuP6I1Ht"
      },
      "source": [
        "### Your answer here. No Python, only Markdown.\n",
        "\n",
        "The Data Ethics Framework guides appropriate and responsible data use in government and the wider public sector. It helps public servants understand ethical considerations, address these within their projects, and encourages responsible innovation. Based on the given context, it is clearly mentioned  that the proposed price recommender algorithm will not be effective. In compliance to this recommender system, i think the comply with the law is the most weakest aspect. According to rule, there must be an understanding of the relevant laws and codes of practice that relate to the use of data. So, i will score 2 out of 5. Furthermore, Review of quality and limitations of data is the another weak factor. One of the reason of such context is lack of data and transparancy. The score for this will be again 2 out of 5. Afterwards Define and understand public benifits and user needs will be in place of 3 alongwith Involvement of diverse expertise and consideration of wider policy implications thereafter.\n",
        "\n",
        "Considering the ethical implications of disclosing information is a major challenge for information providers. On the one hand ,providers have to eveluate the potential ethical or unethical use of disclosed information.\n",
        "\n",
        "\n",
        "Accountability includes effective governance and oversight mechanism for any project. It should be in the hands of the users that they are able to exercise effective oversight and control over decisions taken by gov. But, in this case, due to incomplete data availabiliy, accountability hold weak and users are affected too much. Hence will score 2 out of 5.\n",
        "\n",
        "Finally, the fairness, it is must to avoid project potentials to have unintended discriminatory effects. Try avoiding bias. The authority must be fair enough to showcase the reality of their residents and provide complete robust data. So the score will be somewhere around 3 out of 5.\n",
        "\n",
        "Possibilly there could be multiple solutions in this context. For instance, before getting data from the authority, make sure there is clear articulation of the problem before starting of the project. Also, make sure there must be effective governance structure with experts with all regulations. Hence, it is must to follow ethical way to develop any project and maintain data credibility at same time.\n",
        "\n",
        "To support the above statement used, a statement given by Luciano et al, \"The problem of defining what kind of information should be disclosed by an organisation when the ethical nature of information transparency is taken into account. The common understanding of information transparency as the process of disclosing a set of data has been challenged as too limited, in favour of a more inclusive definition that takes into account also the ethical principles factually endorsed in producing information.\" The points alongwith solutions are in context to discussion with peers as well.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHdqlI5jnUPi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}